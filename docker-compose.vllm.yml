version: '3.10'

services:
  vllm:
    container_name: vllm
    image: vllm/vllm-openai:v0.6.3.post1
    restart: unless-stopped
    ports:
      - "8080:8000"  # Map container port 8000 to host port 8080
    ipc: host
    volumes: 
      - /mnt/models:/app/model
    command: [
      "--model", "/app/model/Meta-Llama-3-8B-Instruct", 
      "--gpu-memory-utilization", "1", 
      "--max-model-len=4096", 
      "--max-num-seqs","2",
      "--tensor-parallel-size", "2",
      "--quantization","fp8",
      "--enforce-eager",
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0','1']
              capabilities: [gpu]
